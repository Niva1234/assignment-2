# assignment-2
XGBoost is a machine learning algorithm. XGBoost stands for extreme gradient boosted decision trees. It is an ensemble method where we take a model and its multiple versions are considered. Each tree boosts attributes that lead to misclassifications of previous tree. A new model is built to predict the errors of existing model. All these predictions are added to ensembles of model. In general, we have multiple trees just building on top of each other to correct the errors of previous trees before it. In XGBoost the data to be in matrix. If the matrix is having a lot of zeros in it, which is known as sparse matrix, the built-in datatype, DMatrix, in XGBoost can store and access this sparse matrix efficiently. After cleansing the data information regarding which training data to be used, how many rounds of training and the objective function has to be mentioned. And XG Boost does linear regression by default. Our initial model had a slightly lower error on our testing data than our training data. By the way over fitting has been avoided. In XG Boost overfitting can be avoided by specifying that you want your decision trees to have fewer layers rather than more layers. That is specifying max.depth.  early_stopping_rounds, that will stop training if we have no improvement in a certain number of training rounds. XGBoost helps in cross validation at each iteration. XGBoost has a lot of built-in functions to help figure out why the model is making the distinctions. Model can be examined by looking at the combination of decision trees. Even though a logistic model is used log odds can be converted into probability easily. By plotting an important matrix important feature can be quickly identified. While representing the tree top of the tree is on the left and the bottom of the tree is on the right. For features, the number next to it is "quality", which helps indicate how important this feature was across all the trees.
